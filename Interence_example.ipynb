{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Inference Workshop -1: Gene Markers and Disease Prediction\n",
    "\n",
    "This workshop demonstrates how Bayesian inference can be used to learn the relationship between gene markers and disease probability. We'll see how our beliefs (represented by probability distributions) update as we gather more data.\n",
    "\n",
    "## Setup\n",
    "First, let's import our required libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from scipy.special import beta as beta_function\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define True Distribution\n",
    "\n",
    "In realistic scenarios, the true probability distribution is unknown. However, for generating synthetic data, we must establish a hypothetical 'true' distribution. In genetics and disease modeling, the probability of a disease given a specific gene marker often varies among individuals rather than being fixed. This variation arises from several factors, including genetic background, environmental influences, and interactions among multiple genes. We model the relationship between our gene marker and the disease probability using a normal distribution, truncated to ensure all probabilities are between 0 and 1. While this approach may not perfectly capture every nuance of biological complexity, it provides a robust approximation that helps estimate the variability and distribution of disease probabilities effectively. This method allows us to simulate data that closely resembles real genetic variability, facilitating more realistic analyses and studies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define true distribution parameters\n",
    "true_mean = 0.3  # 30% probability of disease given marker\n",
    "true_std = 0.05  # Standard deviation of probability\n",
    "\n",
    "def generate_data(n_samples):\n",
    "    \"\"\"Generate binary outcomes (disease/no disease) based on true probability\"\"\"\n",
    "    true_probs = np.random.normal(true_mean, true_std, n_samples)\n",
    "    true_probs = np.clip(true_probs, 0, 1)  # Ensure probabilities are in [0,1]\n",
    "    return np.random.binomial(1, true_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding np.random.binomial\n",
    "\n",
    "\n",
    "The np.random.binomial function in NumPy is used to simulate outcomes from a binomial distribution. This distribution models the number of successes in a given number of trials, each with a specified probability of success.\n",
    "\n",
    "Usage Example:\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "\n",
    "# Assume true_probs is an array of probabilities for disease presence\n",
    "true_probs = np.array([0.3, 0.7, 0.9])\n",
    "```\n",
    "\n",
    "### Simulate disease occurrence: 1 trial per probability with varying success rates\n",
    "```\n",
    "outcomes = np.random.binomial(1, true_probs)\n",
    "```\n",
    "Parameters in Your Code:\n",
    "\n",
    "1: The number of trials for each experiment (since it's 1, each experiment is just one trial).\n",
    "true_probs: An array where each element is the probability of success (disease presence) in that single trial.\n",
    "What Happens:\n",
    "\n",
    "The function performs one trial for each element in the true_probs array.\n",
    "It returns an array of outcomes, where each outcome is either 0 (no disease) or 1 (disease present), based on the probability in true_probs.\n",
    "Output Explanation:\n",
    "\n",
    "Each element of the output corresponds to an outcome of the trial based on its respective probability. For instance, for a probability of 0.3, there's a 30% chance the output will be 1 (disease present) and a 70% chance it will be 0 (no disease).\n",
    "This method is useful for simulating scenarios with different probabilities of an event, allowing for realistic modeling of variable outcomes in a population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior Distribution\n",
    "\n",
    "Now that we have evidence, we need to select a prior distribution for the parameter mean that we are attempting to estimate. Currently, we lack specific information about the distribution of this prior. We can begin with the simplest approach: a uniform distribution. This choice represents complete uncertainty about the relationship between our gene marker and disease probability. However, do not worry; we will demonstrate how, with the accumulation of data, the estimated mean will increasingly converge to the true mean. This reflects a state of complete prior ignorance, where all probabilities from 0 to 1 are considered equally likely, providing a neutral starting point for Bayesian updating based on the data.\n",
    "\n",
    "* Note: When both parameters of the Beta distribution are set to 1, the result is a uniform distribution ([Beta distribution on Wikipedia](https://en.wikipedia.org/wiki/Beta_distribution)).\n",
    "\n",
    "### Beta Distribution as a Prior in Bayesian Statistics\n",
    "\n",
    "The Beta distribution is commonly used as a prior distribution in Bayesian statistics, particularly when modeling probabilities or proportions. Here are the reasons why it is so favored:\n",
    "\n",
    "#### 1. Bounded Support [0, 1]\n",
    "The Beta distribution is defined on the interval [0, 1], which is the natural range for probabilities. This makes it perfectly suited for representing the distribution of a probability value, like the probability of success in a binomial trial.\n",
    "\n",
    "#### 2. Flexibility in Shape\n",
    "The shape of the Beta distribution can vary widely depending on its parameters ($\\alpha$ and $\\beta$):\n",
    "\n",
    "- **Uniform Distribution**: When $\\alpha = 1$ and $\\beta = 1$, the Beta distribution becomes a uniform distribution, reflecting complete ignorance about the probability of an event.\n",
    "- **Skewed Left or Right**: It skews towards 1 when $\\alpha > \\beta$ (suggesting a higher expected probability of success), and towards 0 when $\\beta > \\alpha$ (suggesting a lower expected probability).\n",
    "- **Bell-shaped**: With $\\alpha$ and $\\beta$ greater than 1 and similar in magnitude, it takes a bell-shaped form, centered around $\\frac{\\alpha}{\\alpha + \\beta}$.\n",
    "\n",
    "#### 3. Conjugacy with Binomial Likelihood\n",
    "The Beta distribution is a conjugate prior for the binomial likelihood, which means:\n",
    "- The posterior distribution, after observing outcomes of binomial trials, will also be a Beta distribution.\n",
    "- This conjugacy allows for straightforward updating of the distribution:\n",
    "  - **Posterior's $\\alpha$**: Prior's $\\alpha$ + Number of Successes\n",
    "  - **Posterior's $\\beta$**: Prior's $\\beta$ + Number of Failures\n",
    "\n",
    "#### 4. Analytical Convenience\n",
    "Due to its conjugacy properties, updates to the Beta distribution in light of new data can often be done analytically, avoiding the need for numerical approximation methods like Monte Carlo simulations.\n",
    "\n",
    "#### 5. Interpretability\n",
    "The parameters of the Beta distribution can be interpreted as \"pseudo-observations\":\n",
    "- For example, $\\alpha = 10$ and $\\beta = 5$ might represent having seen 10 successes and 5 failures before any real data collection.\n",
    "\n",
    "#### Usage Examples\n",
    "- **Medical Trials**: Used to model the probability of a patient responding to treatment.\n",
    "- **Quality Control**: Applied in industrial statistics to model defect rates in manufacturing processes.\n",
    "- **Machine Learning**: Employed in Bayesian models for binary outcomes and in online learning algorithms for dynamically updating success probabilities.\n",
    "\n",
    "The Beta distribution is a key tool in Bayesian analysis, especially useful when dealing with probabilities and proportions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prior parameters (Beta distribution) prior_a and prior_b are the initial parameters of the Beta distribution, known as alpha (α) and beta (β), respectively. When both are set to 1, the Beta distribution is equivalent to a uniform distribution over the interval [0, 1], representing complete uncertainty or lack of knowledge about the probability being modeled.\n",
    "prior_a = 1  # Alpha parameter for Beta distribution\n",
    "prior_b = 1  # Beta parameter for Beta distribution\n",
    "\n",
    "def update_beta_parameters(a, b, data):\n",
    "    \"\"\"Update Beta distribution parameters with new data\"\"\"\n",
    "    return a + np.sum(data), b + len(data) - np.sum(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Functions\n",
    "\n",
    "Let's create functions to visualize how our distributions evolve with more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distributions(data_points, prior_a, prior_b, true_mean, true_std, ax=None):\n",
    "    \"\"\"Plot prior, likelihood, and posterior distributions\"\"\"\n",
    "\n",
    "    x = np.linspace(0, 1, 1000)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot prior\n",
    "    prior = stats.beta(prior_a, prior_b)\n",
    "    plt.plot(x, prior.pdf(x), 'b-', label='Prior (Uniform)', alpha=0.5)\n",
    "\n",
    "    \n",
    "    # Plot posterior if we have data\n",
    "    if len(data_points) > 0:\n",
    "        post_a, post_b = update_beta_parameters(prior_a, prior_b, data_points)\n",
    "        posterior = stats.beta(post_a, post_b)\n",
    "        plt.plot(x, posterior.pdf(x), 'r-', \n",
    "                label=f'Posterior (n={len(data_points)})', \n",
    "                alpha=0.7)\n",
    "    \n",
    "    # add a line as the true mean\n",
    "    plt.axvline(true_mean, color='g', linestyle='--', label='True Mean Parameter')\n",
    "    \n",
    "    plt.title(f'Distribution Comparison (n={len(data_points)})')\n",
    "    plt.xlabel('Probability of Disease Given Gene Marker')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics if we have data\n",
    "    if len(data_points) > 0:\n",
    "        post_a, post_b = update_beta_parameters(prior_a, prior_b, data_points)\n",
    "        print(f\"Posterior mean: {post_a/(post_a + post_b):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data and Visualize Results\n",
    "\n",
    "Now let's generate synthetic data and see how our posterior distribution evolves with different sample sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all data at once\n",
    "all_data = generate_data(1000)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i, n in enumerate([1, 2, 5, 10, 100, 1000]):\n",
    "    plot_distributions(all_data[:n], prior_a, prior_b, true_mean, true_std)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence Analysis\n",
    "\n",
    "Let's visualize how our estimates converge to the true probability over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_convergence(data, true_mean):\n",
    "    \"\"\"Plot convergence of estimates to true probability\"\"\"\n",
    "    running_means = []\n",
    "    posterior_means = []\n",
    "    ns = range(1, len(data) + 1)\n",
    "    \n",
    "    current_a = prior_a\n",
    "    current_b = prior_b\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        running_means.append(np.mean(data[:i+1]))\n",
    "        current_a, current_b = update_beta_parameters(prior_a, prior_b, data[:i+1])\n",
    "        posterior_means.append(current_a / (current_a + current_b))\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(ns, running_means, 'b-', label='Sample Mean', alpha=0.5)\n",
    "    plt.plot(ns, posterior_means, 'r-', label='Posterior Mean', alpha=0.7)\n",
    "    plt.axhline(y=true_mean, color='g', linestyle='--', label='True Mean')\n",
    "    plt.xlabel('Number of Samples')\n",
    "    plt.ylabel('Probability Estimate')\n",
    "    plt.title('Convergence of Estimates to True Probability')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Plot convergence\n",
    "plot_convergence(all_data, true_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Key observations from this workshop:\n",
    "\n",
    "1. **Prior Distribution**:\n",
    "   - We started with a uniform prior (Beta(1,1)), representing complete uncertainty\n",
    "   - This is an uninformative prior that gives equal probability to all possible values\n",
    "\n",
    "2. **True Distribution**:\n",
    "   - The true distribution is normal with mean=0.3 and std=0.05\n",
    "   - This represents the actual probability of disease given the gene marker\n",
    "\n",
    "3. **Posterior Evolution**:\n",
    "   - With 10 samples: Initial shape begins to form but still quite uncertain\n",
    "   - With 100 samples: Much closer to true distribution, narrower confidence interval\n",
    "   - With 1000 samples: Very close to true distribution, tight confidence interval\n",
    "\n",
    "4. **Convergence**:\n",
    "   - The posterior mean converges to the true mean as we get more data\n",
    "   - The variance of the posterior distribution decreases with more data\n",
    "   - This demonstrates how Bayesian updating allows us to learn from data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
